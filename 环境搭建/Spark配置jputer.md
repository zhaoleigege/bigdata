# Spark配置jputer

在计算机中安装好java、scala、python的开发环境并设置好环境变量，下载spark和hadoop并添加如下的环境变量

```shell
export SPARK_HOME="/Users/buse/Documents/spark"
export PATH="/Users/buse/Documents/spark/bin:$PATH"

export HADOOP_HOME="/Users/buse/Documents/hadoop"
```





#### 参考资料

* [spark集成jupter](https://www.davidadrian.cc/posts/2017/08/how-to-spark-cluster/)
* [源码编译hadoop](http://www.ercoppa.org/posts/how-to-compile-apache-hadoop-on-ubuntu-linux.html)
* [How to run Scala and Spark in the Jupyter notebook](https://medium.com/@bogdan.cojocar/how-to-run-scala-and-spark-in-the-jupyter-notebook-328a80090b3b)